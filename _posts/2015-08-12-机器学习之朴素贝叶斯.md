---
layout: post
category: Machine Learning
title: 机器学习之朴素贝叶斯
tagline: by Archer
tags: [机器学习]
---
最为广泛的两种分类模型是决策树模型(Decision Tree Model)和朴素贝叶斯模型（Naive Bayesian Model，NBM）

朴素贝叶斯法是基于贝叶斯定理与特征条件独立假设的分类方法
<!--more-->

##贝叶斯决策理论
假设我们现在有一个数据集，由两类数据组成

用p1(x,y)表示数据点(x,y)属于类别1的概率，p2(x,y)表示属于类别2的概率，那么根据贝叶斯决策理论，可以用下面的规则判断他的类别：

*p1(x,y) > p2(x,y) 则类别为1，否则类别为2*


##条件概率和贝叶斯准则

NBC朴素贝叶斯主要是基于条件概率

贝叶斯准则告诉我们如何交换条件概率中的条件与结果，即如果已知P(x\|c),求P(c\|x)，可以使用以下计算方法：

![Yanghu](/assets/2015-08-11-机器学习之朴素贝叶斯/1.png)

之前提到的贝叶斯决策理论要求计算两个概率p1(x,y)和p2(x,y)只是为了简化描述，其实真正需要计算的是p(c1\|x,y)以及p(c1\|x,y),代表的具体意义是，给定由x，y表示的数据点，那么该数据点来自于c1或者c2的概率是多少？

根据贝叶斯准则，我们可以得到：

![Yanghu](/assets/2015-08-11-机器学习之朴素贝叶斯/2.png)

便可以用三个可以求出的概率来推出未知概率

##应用：使用朴素贝叶斯进行文档分类
做一个检查英文句子中是否含有侮辱性词汇的自动分类器

首先我们可以遍历输入的训练集，来构建一个所有出现过的词的词汇表。我们可以观察文档中出现的词，把每个词的出现或者不出现作为一个特征，这样得到得特征数目就会跟词汇表中的词目一样多。

朴素贝叶斯的另一个假设是，每个特征同等重要，即特征之间相互独立，所以假设每个特征需要n个样本，那么1000个特征的词汇表需要1000N个样本，如果特征之间不互相独立，所需的可能就是N的1000次方个样本了

以下代码构建了词汇表，以及每个(句)实例对应词汇表的词向量

{% highlight python %}
  3 def loadDataSet():
  4      postingList=[['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'],
  5              ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],
  6              ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],
  7              ['stop', 'posting', 'stupid', 'worthless', 'garbage'],
  8              ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],
  9              ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]
 10      classVec = [0,1,0,1,0,1]    #1 is abusive, 0 not
 11      return postingList,classVec
 12
 13 #构建一个字典
 14 def createVocabList(dataSet):
 15      vocabSet = set([])
 16      for document in dataSet:
 17          vocabSet = vocabSet | set(document)
 18      return list(vocabSet)
 19
 20 #将一个输入字符串转换成字典向量形式
 21 def setOfWordsToVec(vocabList, inputSet):
 22     returnVec = [0] * len(vocabList)
 23     for word in inputSet:
 24         if word in vocabList:
 25             returnVec[vocabList.index(word)] = 1
 26         else:
 27             print "The word: %s is not in my vocabulary!" % word
 28     return returnVec

{% endhighlight %}

##训练算法：从词向量计算概率

由于特征的数目已经和词汇表向量的维数一样，我们改写之前的贝叶斯准则，将x，y替换为*w*，表示特征的向量

![Yanghu](/assets/2015-08-11-机器学习之朴素贝叶斯/3.png)

我们对每个类别计算该值，然后比较类别之间概率的大小。

比较时，两边的w一致，所以不用算

首先计算p(Ci)，直接根据classVec就可以求出，每个类别在训练集中出现的概率


接下来是p(w\|Ci)，这里就要用到朴素贝叶斯假设，将*w*展开为一个个独立的特征，那么就可以写作p(*w1,w2,w3..wn*\|Ci),假设所有词相互独立，那么意味着可以使用p(w1\|Ci)p(w2\|Ci)...来计算概率

p(w1\|ci):举例来说，假设w1为 stupid，ci 为侮辱性词汇(Abusive)那么p(stupid\|Abusive) 即侮辱性词句中stupid一词出现的概率，可以统计训练集中侮辱性词句中出现stupid的次数，去除以侮辱性词句的单词总数，即可得到该概率

训练代码如下，得到p(w|ci)
{% highlight python %}
 31 def trainNB0(trainMatrix, trainCategory):
 32     #实例的数目
 33     numTrainDocs = len(trainMatrix)
 34     #词汇表的数目
 35     numWords = len(trainMatrix)
 36     #p(Ci)
 37     pAbusive = sum(trainCategory)/float(numTrainDocs)
 38     p0Num = zeros(numWords)
 39     p1Num = zeros(numWords)
 40     p0Sum = 0.0
 41     p1Sum = 0.0
 42     for i in range(numTrainDocs):
 43         if trainCategory[i] == 1:
 44             #矩阵相加，每个词的数目加1
 45             p1Num += trainMatrix[i]
 46             #类别为1的词数目增加
 47             p1Sum += sum(trainMatrix[i])
 48         elif trainCategory[i] == 0:
 49             p0Num += trainMatrix[i]
 50             p1Sum += sum(trainMatrix[i])
 51     #得到每一个词的p(wi|ci)构成的矩阵p(w|ci)
 52     p1Vec = p1Num / p1Sum
 53     p0Vec = p2Num / p2Sum
 54     return p0Vec, p1Vec, pAbusive
{% endhighlight %}

##优化

*	在使用分类器的时候，将会计算p(w0\|c1)p(w2\|c1)...某一项为0时整个乘积则会为0，为了降低这种影响可以把

	p0Num = zeros(numWords) =>  p0Num = ones(numWords)

同时将分母也初始化为2
	
	p0Sum = 2.0 ...
	
*	另一个问题是下溢出，在乘的时候由于很多因子都非常小，程序会下溢得到不正常的答案，线性代数中有这样的规律

	ln(a*b) = ln(a) + ln(b)

使用该公式，我们可以使用自然对数对曲线进行处理

	p1Vec = log(p1Num/p1Sum) 

*	目前为止我们都是将每个词的出现与否作为一个特征，这可以被描述为*词集模型*，如果一个词在文档中出现不止一次，可能意味着包含着该词是否出现在文档中所不能表达的某种信息，这种方法被称为*词袋模型*

具体实现就是将构建词向量函数中的

	 returnVec[vocabList.index(word)] = 1
	 
变为  

	 returnVec[vocabList.index(word)] += 1


##分类算法

在得到了p0V,p1V以及pClass之后，我们就可以对某个特征向量进行分类了

{% highlight python %}
def classifyNB(vec2Verify, p0Vec, p1Vec, pClass):
	 #log(p(w|c1) * p(c1)) = log[p(w|c1)] + log[p(c1)]
     p1 = sum(vec2Verify * p1Vec) + log(pClass)
     p0 = sum(vec2Verify * p0Vec) + log(1 - pClass)
     if(p1 > p0):
         return 1
     else:
         return 0
{% endhighlight %}

##应用：使用朴素贝叶斯过滤垃圾邮件

之前的内容中，训练集都是预先准备好的，在真实的情况中，需要我们自己对文本进行切分处理，产生自己的数据集，如下一个函数代表了最简单的一种文本处理方式

代码去掉小于2的单词，并且将所有单词转为小写，以及单词按空格和标点符号切分
{% highlight python %}
    def textParse(bigString):
       import re
       listOfTokens = re.split(r'\W*', bigString)
       return [tok.lower() for tok in listOfTokens if len(tok) > 2]
{% endhighlight %}

有了上面的函数，我们的工具库就基本搭建完成了，假设我们有25封邮件归为spam,25封邮件归为ham（非垃圾），可以进行如下几步来进行分类：

1.	将50个文件读入，使用createVocabList构建单词表。
对于每个文件，使用textParse将其变为字符列表，使用setOfWordsToVec函数将其变为单词向量

2.	使用训练算法trainNB0随机选取其中40个向量进行训练，得到训练后的p0V,p1V以及pClass1

3.	对于剩余的10个向量，使用classifyNB对其进行判断，分类结果与其实际class进行对比，统计错误率

2，3步选择向量可以是随机的，可以通过多次运行程序获得平均错误率

应用代码如下：

{% highlight python %}
	def spamTest():
 62     docList = []; classList = []; fullText = []
 63     for i in range(1,26):
 64         wordList = textParse(open('email/spam/%d.txt' % i).read())
 65         docList.append(wordList)
 66         fullText.extend(wordList)
 67         classList.append(1)
 68         wordList = textParse(open('email/ham/%d.txt' % i).read())
 69         docList.append(wordList)
 70         fullText.extend(wordList)
 71         classList.append(0)
 72     vocabList = createVocabList(docList)
 73      #只通过数字记录哪些是training哪些是test
 74     trainingSet = range(50)
 75     testSet = []
 76     #从TrainingSet训练集中随机取10个用于TestSet测试集
 77     for i in range(10):
 78         randIndex = int(random.uniform(0,len(trainingSet)))
 79         testSet.append(trainingSet[randIndex])
 80         del(trainingSet[randIndex])
 81         #开始训练
 82         trainMat = []; trainClasses = []
 83     for trainIndex in trainingSet:
 84         #将每一个训练集实例的内容根据词汇表变为向量
 85         trainMat.append(setOfWordsToVec(vocabList,docList[trainIndex]))
 86         trainClasses.append(classList[trainIndex])
 87
 88         p0V,p1V,pSpam = trainNB0(array(trainMat), array(trainClasses))
 89         errorCount = 0
 90
 91     for testIndex in testSet:
 92         wordVector = setOfWordsToVec(vocabList, docList[testIndex])
 93         if classifyNB(array(wordVector),p0V,p1V,pSpam) != classList[testIndex]:
 94             errorCount += 1
 95     print "Error rate is : " , float(errorCount) / len(testSet)

{% endhighlight %}

书中还有一个从个人广告中获取区域倾向的例子，先留着没看，打算有点积累了再回来仔细研究
