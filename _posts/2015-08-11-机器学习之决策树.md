---
layout: post
category: Machine Learning
title: 机器学习之决策树
tagline: by Archer
tags: [机器学习]
---

决策树学习是一种逼近离散值目标函数的方法，在这种方法中学习到的函数表示为一颗决策树

<!--more-->

决策树通过把实例从根节点排列到某个叶子节点来分类实例（某个元素），叶子节点即为实例所属的分类。
树上的每一个节点指定了对实例的某个属性的测试，并且该节点后的每一个分支对应于该属性的一个可能值
![Yanghu](/assets/2015-08-11-机器学习之决策树/决策树.jpg)

##ID3算法
在构造决策树时，我们需要解决的第一个问题就是，*当前数据集上那个特征在划分数据分类时起决定作用*

我们必须评估每个特征，得出一个最优的特征，根据这个特征进行的子集划分 *更加有序*，这就需要用到信息论中的一些知识

划分数据的最大原则是将无序的数据变得更加有序，有序的信息集合，*熵*会小一些。熵的定义为信息的期望值，明白这个词我们必须先要了解信息的定义，如果待分类的食物可能
划分在多个分类之中，则该事物在某分类（类别）Xi 上的信息定义为

	l(Xi)= -log2p(Xi) p(Xi)为选择该分类的概率
	
熵是该事物所有类别所有可能值包含的信息的期望值，以下公式可以得到熵
	
![Yanghu](/assets/2015-08-11-机器学习之决策树/熵.jpg)

以下是计算熵的函数：

{% highlight python %}

 15 def calcShannonEnt(dataSet):
 16     numOfEntries = len(dataSet)
 17     labelCounts = {}
 18     for featureVector in dataSet: #先统计该数据集中一共有几个类别，并统计出现次数
 19         currentLabel = featureVector[-1]
 20         if currentLabel not in labelCounts.keys():
 21             labelCounts[currentLabel] = 0
 22         labelCounts[currentLabel] += 1
 23     shannonEnt = 0.0
 24     for key in labelCounts.keys(): #香农熵求解
 25         prob = float(labelCounts[key])/numOfEntries
 26         shannonEnt -= prob * log(prob, 2)
 27     return shannonEnt

{% endhighlight %}

有了计算数据集熵的函数，接下来要做的就是依次计算按某个特征进行划分所得到子集的熵，并且求出子集熵之和最小的那个特征

首先要写一个函数，根据某个特征的某个值，将数据集进行子集划分

axis为特征的位置，value为特征的取值
{% highlight python %}

 35 def splitDataSet(dataSet, axis, value):
 36     splitedDataSet = []
 37     for featureVector in dataSet:
 38         if featureVector[axis] == value:
 39             reducedFeatureVector = featureVector[:axis]
 40             #extend为扩充数组内容
 41             reducedFeatureVector.extend(featureVector[axis + 1:])
 42             #append可以讲一个列表作为元素追加在尾部
 43             splitedDataSet.append(reducedFeatureVector)
 44     return splitedDataSet
{% endhighlight %}

有了该方法后，可以在数据集中寻找最合适的划分特征
{% highlight python %}

 46 def findBestFeature(dataSet):
 47     #使用range来遍历，因此要-1
 48     numOfFeatures = len(dataSet[0]) - 1
 49     bestFeature = -1
 50     baseEntrophy = calcShannonEnt(dataSet)
 51     #对于每一个Feature
 52     for i in range(numOfFeatures):
 53         featureList = [example[i] for example in dataSet]
 54         featureSet = set(featureList)
 55         newShannonSum = 0.0
 56         #按照该Feature将原有dataSet分为subSet
 57         for value in featureSet:
 58             subDataSet = splitDataSet(dataSet, i, value)
 59             oneSum = calcShannonEnt(subDataSet)
 60             prob = len(subDataSet)/float(len(dataSet))
 61             newShannonSum += oneSum * prob
 62         #比较新的熵与原有数据集的熵
 63         if newShannonSum < baseEntrophy:
 64             baseEntrophy = newShannonSum
 65             bestFeature = i
 66     return bestFeature、
 
{% endhighlight %}

有了如上这些函数，就可以进行决策树的构建了，决策树构建的伪代码如下：
![Yanghu](/assets/2015-08-11-机器学习之决策树/决策树伪代码.png)

工作原理总结如下：基于最好（子集熵最小）的属性值划分数据集，由于特征值可能多于2个，因此可能存在多余2个分支的子集划分，第一次划分之后，数据将被向下传递到树分支的下一个节点，在这个节点上我们可以再次划分数据，因此在这里应该用到递归

递归结束的条件是：程序遍历玩所有划分数据集的特性（子集中只有一个特性），或者是子集中的所有实例都具有相同的分类

由于有的情况下，直到程序遍历完所有特性（子集中仅剩一个特性）后，子集中的分类仍然不统一，此时我们需要决定如何定义该叶子节点的分类，我们在这里采用多数表决法

{% highlight python %}

  5 def findMajorClass(classList):
  6     classCount = {}
  7     for oneClass in classList:
  8         if oneClass not in classCount.keys():
  9             classCount[oneClass] = 0
 10         classCount[oneClass] += 1
 11     sortedClassCount = sorted(classCount.iteritems(), key= ope    rator.itemgetter(1), reverse=True)
 12     return sortedClassCount[0][0]

{% endhighlight %}

创建树的算法如下：

{% highlight python %}

 68 def createTree(dataSet, labels):
 69     classList = [example[-1] for example in dataSet]
 70     #如果dataSet所有元素的分类都完全相同，无法再分，    将这种分类返回
 71     if classList.count(classList[0]) == len(classList):
 72         return classList[0]
 73     #如果dataSet中的元素只剩下一个特性，此时可能分类完全一致    或者有多数有少数，采用多数表决制，返回该dataSet中的代表性分类
 74     if len(classList[0]) == 1:
 75         return findMajorClass(dataSet)
 76     bestFeature = findBestFeature(dataSet)
 77     bestFeatureLabel = labels[bestFeature]
 78     myTree = {bestFeatureLabel:{}}
 79     del(labels[bestFeature])
 80     #开始构建这一层的树
 81     uniqueValuesInBestFeature= set([example[bestFeature] for example in dataSet])
 82     for value in uniqueValuesInBestFeature:
 83          subLabels = labels[:]
 84          myTree[bestFeatureLabel][value] = createTree(splitDat    aSet(dataSet, bestFeature, value), subLabels)
 85     return myTree
{% endhighlight %}

测试数据:
	dataSet = [[1,1,'yes'],[1,1,'yes'],[1,0,'no'],[0,1,'no'],[    0,1,'no']]
	labels = ['no surfacing', 'flippers']

测试结果：
{'no surfacing': {0: 'no', 1: {'flippers': {0: 'no', 1: 'yes'}}}}

需要注意的是，决策树中的特征以及最终的分类都需要是标称型的，不能是连续性的

其余有关决策树的可视化以及应用还没有耐心看下去，将来补充
