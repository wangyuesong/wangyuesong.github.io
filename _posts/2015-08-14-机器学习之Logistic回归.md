---
layout: post
category: Machine Learning
title: 机器学习之Logistic Regression
tagline: by Archer
tags: [机器学习]
---

## Linear Regression线性回归
在线性回归中，模型Hypothesis为：
<!--more-->
![Yanghu](/assets/2015-08-14-机器学习之Logistic回归/Linear Regression Hypothesis.png)

Cost Function为：
![Yanghu](/assets/2015-08-14-机器学习之Logistic回归/Linear Regression Cost Function.png)

线性回归的目标就是求出一组合适的参数使得这个Cost Function最小，要注意的是，这个Cost Function的自变量是theta，x是输入的dataSet，y是真实值，所以该这是一个关于(theta1,theta2...thetaN)的一个函数,
其中x0 被默认设置为1

在Andrew Ng的课中，想求出这4个函数的最小值，需要通过Gradient Descent的方法，也就是梯度下降法，梯度下降法的思想是，进行多次循环，每一次循环都会去更新参数向量theta，具体公式如下：

![Yanghu](/assets/2015-08-14-机器学习之Logistic回归/Linear Regression Gradient Descent.png)

其中偏导数部分根据链式求导法则可以得到：
![Yanghu](/assets/2015-08-14-机器学习之Logistic回归/Linear Regression Cost Function Partial Derivative.png)

假设现在有一些数据点，我们用一条直线对这些点进行拟合，这个拟合过程就被称作回归，利用Logister回归进行分类的主要思想是，根据现有数据对分类边界线建立回归公式，以此进行分类。


但是得到回归公式之后，该怎么利用回归公式进行分类呢？我们想得到的函数应该是接受一个输入然后预测出类别，比如，在只有两个类的情况下，函数可以输出0或者1。Sigmoid函数可以完成这样的功能，函数计算公式如下：

![Yanghu](/assets/2015-08-14-机器学习之Logistic回归/1.png)

函数的曲线如下，当x刻度很大时，sigmoid很像一个阶跃函数

![Yanghu](/assets/2015-08-14-机器学习之Logistic回归/2.png)

因此，为了实现Logistic回归分类器，我们可以
1.	将每个特征都乘以一个回归系数，然后结果值相加
2.	将总和带入Sigmoid函数，进而得到一个0-1之间的数值
3.	0-0.5归入0类，大于0.5归入1类

因此Logistic回归也可以被认为是一种概率估计，现在的问题在于求出最佳回归系数

#基于最优化方法的最佳回归系数确定

Sigmoid函数的输入记为z，可以由下面的公式得出:

	z= w0x0 + w1x1 ... wnxn

全部采用向量写法，可以写成z = wT x 也就是特征向量和系数向量的内积

###梯度上升法
梯度上升法是求函数最大值的一种最优化方法，思想是：要找到某函数的最大值，最好的方式是沿着该函数的梯度方向搜寻，函数f（x，y）的梯度由如下表示:

![Yanghu](/assets/2015-08-14-机器学习之Logistic回归/3.png)

需要记住的是梯度是一个向量，是在定义域所在平面的一个向量，梯度方向是函数值增长最快的方向

如此我们得到梯度，可以确定搜寻的方向，而移动量的大小需要我们设定参数来确定，梯度算法的迭代公式如下

![Yanghu](/assets/2015-08-14-机器学习之Logistic回归/4.png)

该公式将一直迭代执行直到到达某个停止条件为止，比如迭代次数到达某个指定值或者算法到达某个可允许的误差范围

我们的Sigmoid函数的输入是w0x0 + w1x1... wnxn，在使用梯度上升求最大值时，上图中的w是一个向量（w1，w2... wn)

