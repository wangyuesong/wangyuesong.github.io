---
layout: post
category: Machine Learning
title: 机器学习之kNN分类器
tagline: by Archer
tags: [机器学习]
---

kNN算法是一种监督学习算法，容易理解，过程为：

<!--more-->

1. 选择一种距离计算方式（欧式距离），计算待分类新数据的所有特征与已知类别数据集中数据点的距离
2. 按距离递增依次排序，选取与当前距离最小的k个点
3. 对于离散分类，返回k个点中出现频率最多的点的类别；对于连续（回归）则采用k个点的加权值作为预测值（距离反比加权）

kNN算法关键在于：

1. 数据的所有特征都要进行可比较的量化，比如特征为颜色（红，黑，蓝），其本身是无法进行距离比较的，可以转化为灰度值再进行比较
2. 样本特征的加权，样本的每一个特征（参数）都有自己的定义域和取值范围，其对计算的影响也不一样，因此需要scale处理（归一化处理）
3. 距离的定义，距离的定义有很多，如欧氏距离、余弦距离、汉明距离、曼哈顿距离等等，选择合适的距离定义将显著提高算法分类的准确性
4. 确定k值，多采用交叉验证法，选择数据样本的一部分作为训练样本，一部分作为测试样本。通过训练数据训练一个机器学习模型，
然后利用测试数据测试其误差率。 cross-validate（交叉验证）误差统计选择法就是比较不同K值时的交叉验证平均误差率，选择误差率最小的那个K值。

{% highlight python %}
   # -*- coding: UTF-8 -*-
   from numpy import *
   import operator
  
   def createDataSet():
       group = array([[1.0,1.1],[1.0,1.0],[0,0],[0,0.1]])
       labels = ['A','A','B','B']
       return group,labels
  
   def classify0(inX, dataSet, labels, k):
       #numpy 函数，获得矩阵第一维度的长度
       dataSetSize = dataSet.shape[0]
       
       #矩阵相减
       #将inX 复制dataSize个，1代表原样复制，不补0，如果是2则将前面补2个0再复制
       diffMat = tile(inX, (dataSetSize,1)) - dataSet
       
       #每个平方
       sqDiffMat = diffMat ** 2
       
       #按行求和
       sqDistances = diffMat.sum(axis=1)
       
       #开根号
       sqDistances = sqDistances ** 0.5
       
       #numpy函数，按数组内容大小排序，但是排出的是对应的下标
       sortedSqDistanceIndices = sqDistances.argsort()
       
       #创建一个字典记录前k个最近距离样本中，类别出现的个数
       classCount = {}
       for i in range(k):
           voteLabel = labels[sortedSqDistanceIndices[i]]
           classCount[voteLabel] = classCount.get(voteLabel,0) + 1
           
       #sorted函数，第一个参数放一个iterable的对象，key为一个函数，获取遍历对象的第1个域的值
       sortedClassCount = sorted(classCount.iteritems(), key=operator.itemgetter(1),reverse=True)
       
       #返回排序后vote最多的点的label
       return sortedClassCount[0][0]
 
   if __name__ == '__main__':
      dataSet = createDataSet()
      target = classify0([2,2], dataSet[0], dataSet[1], 3)
      print(target)
{% endhighlight %}